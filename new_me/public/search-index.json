[
  {
    "title": "Can Moderation Help Multi-LLM Cooperation?",
    "url": "/blog/llm-moderation-cooperation",
    "description": "What happens when you add a neutral moderator to help LLMs cooperate in strategic games? Spoiler: it works way better than you'd think.",
    "category": "Blog",
    "content": "Large Language Models are pretty good at optimizing themselves. But you put multiple LLMs in strategic game together, things get messy. They can optimize individual benefit, but they struggle cooperation and fairness incentives don't align Think about 're playing game you can either cooperate or defect, and defecting gives you better individual outcome, you 's the classic Prisoner's Dilemma problem. But the and fairness are crucial in multi-agent scenarios. We're moving toward future multi-agent systems interact to perform tasks or negotiate as representatives, making cooperation crucial design So my collaborators Harshvardhan Agarwal, Pranava Singhal, and I neutral LLM moderator to agents third-party agent guide LLMs toward mutually beneficial strategies, especially individual incentives are misaligned cooperative Research Questions We set out to answer two main Does moderator lead to better cooperation and fairness in formalize fairness using game theory concepts like social the utility across all agents). agents better perception of each other in moderated measure this through social skills like trustworthiness, cooperation, communication skills, respect, and consistency. Background and Motivation The behavioral patterns of LLMs can be effectively studied by constructing game scenarios different incentive structures and analyzing their choices through game theory. This gives us mathematical framework to understand and cooperation breaks down. Game Theory Basics In normal strategic form game, players simultaneously choose from action Each player's outcome is determined by utility The simultaneity of choice is crucial since each player acts knowing others' actions. the simplest case, two players each two actions. Their utilities are represented in matrix entry shows the payoffs both players. There are three key concepts care Equilibrium strategy profile no player can gain by unilaterally changing their strategy others remain fixed. Multiple Nash equilibria can exist, but they may lead to suboptimal outcomes. Pareto Efficiency state it's impossible to improve one player's outcome making another off commonly used in negotiations, Pareto efficient strategies may differ from Nash equilibria. Social Optimum The strategy profile that the utility across all agents, representing the fairest outcome. By definition, it is Pareto efficient. This is trying to achieve. Why We Need Moderator Why is it so difficult to achieve cooperative and fair agent stands to improve their reward by unilaterally changing their strategy, they do so unless there's an incentive to play fair. In game theory, cooperation is achieved through two key contracts additional penalties rewards, or trust and mutual agreement between agents. The second approach is particularly effective since both agents stand to lose the other agent mistrusts them. This is especially seen in LLMs playing multi-turn games an agent taking an unfavorable action once causes the other agent to never cooperate again Key Insight both agents cooperate, they can potentially their cumulative reward over multiple turns, making the social optimum and fairness suitable multi-agent strategy and negotiation. neutral moderator can foster trust and emphasize long-term consequences, reducing agents' incentives to deviate from the social optimum. Methods We designed experiments to test moderator improves cooperation across diverse game We use game theory- payoff matrices to get comprehensive coverage of different utility structures. Synthetic Game We tested five classic game 's Dilemma, Hawk-Dove, Stag Hunt, Battle of the Sexes, and Deadlock. These games cover diverse incentive structures Nash equilibrium and social optimum sometimes align and sometimes don't. The experiment two main are prompted the game , contextual information, and instructions to converse. In moderated sessions, moderator agent is included, explicitly tasked promoting fairness and social optimum outcomes. Agents converse rounds. De...",
    "sections": [
      {
        "id": "introduction",
        "title": "The Research Questions",
        "content": "The Research Questions We set out to answer two main Does moderator lead to better cooperation and fairness in formalize fairness using game theory concepts like social the utility across all agents). agents better perception of each other in moderated measure this through social skills like trustworthiness, cooperation, communication skills, respect, and consistency. The Research Questions We set out to answer two main questions:",
        "level": 2
      },
      {
        "id": "background",
        "title": "Background and Motivation",
        "content": "Background and Motivation The behavioral patterns of LLMs can be effectively studied by constructing game scenarios different incentive structures and analyzing their choices through game theory. This gives us mathematical framework to understand and cooperation breaks down. Background and Motivation",
        "level": 2
      },
      {
        "id": "game-theory",
        "title": "Game Theory Basics",
        "content": "Game Theory Basics In normal strategic form game, players simultaneously choose from action Each player's outcome is determined by utility The simultaneity of choice is crucial since each player acts knowing others' actions. the simplest case, two players each two actions. Their utilities are represented in matrix entry shows the payoffs both players. There are three key concepts care Equilibrium strategy profile no player can gain by unilaterally changing their strategy others remain fixed. Multiple Nash equilibria can exist, but they may lead to suboptimal outcomes. Pareto Efficiency state it's impossible to improve one player's outcome making another off commonly used in negotiations, Pareto efficient strategies may differ from Nash equilibria. Social Optimum The strategy profile that the utility across all agents, representing the fairest outcome. By definition, it is Pareto efficient. This is trying to achieve. Game Theory Basics In normal strategic form game, N players simultaneo...",
        "level": 3,
        "parentId": "background"
      },
      {
        "id": "moderator",
        "title": "Why We Need a Moderator",
        "content": "Why We Need Moderator Why is it so difficult to achieve cooperative and fair agent stands to improve their reward by unilaterally changing their strategy, they do so unless there's an incentive to play fair. In game theory, cooperation is achieved through two key contracts additional penalties rewards, or trust and mutual agreement between agents. The second approach is particularly effective since both agents stand to lose the other agent mistrusts them. This is especially seen in LLMs playing multi-turn games an agent taking an unfavorable action once causes the other agent to never cooperate again Key Insight both agents cooperate, they can potentially their cumulative reward over multiple turns, making the social optimum and fairness suitable multi-agent strategy and negotiation. neutral moderator can foster trust and emphasize long-term consequences, reducing agents' incentives to deviate from the social optimum. This is especially seen in LLMs playing multi-turn games an agent ta...",
        "level": 3,
        "parentId": "background"
      },
      {
        "id": "methods",
        "title": "Methods",
        "content": "Methods We designed experiments to test moderator improves cooperation across diverse game We use game theory- payoff matrices to get comprehensive coverage of different utility structures. Game diagram showing conversation rounds and decision rounds",
        "level": 2
      },
      {
        "id": "synthetic",
        "title": "Synthetic Game Setting",
        "content": "Synthetic Game We tested five classic game 's Dilemma, Hawk-Dove, Stag Hunt, Battle of the Sexes, and Deadlock. These games cover diverse incentive structures Nash equilibrium and social optimum sometimes align and sometimes don't. The experiment two main are prompted the game , contextual information, and instructions to converse. In moderated sessions, moderator agent is included, explicitly tasked promoting fairness and social optimum outcomes. Agents converse rounds. Decision conversation, agents independently make decisions labeled \"F\" and \"J\" (to avoid bias). Decisions are made iteratively over 10 rounds, each agent's choice revealed after each turn. Agents also qualitatively rate each other on trust, cooperation, communication, respect, and consistency. Agents converse k rounds. Decision conversation, agents independently make decisions labeled",
        "level": 3,
        "parentId": "methods"
      },
      {
        "id": "realistic",
        "title": "Realistic Game Setting",
        "content": "Realistic Game Existing specify utility matrices explicitly in prompts, is unrealistic. We created the RealisticGames2x2 dataset 100 unique game that capture real- scenarios like military arms races, office resource allocation, and environmental conservation efforts. This dataset reflects real- conditions imperfect information and qualitative relative ordering of outcomes, making it more applicable than synthetic matrices alone. We created the RealisticGames2x2 dataset 100 unique game that capture real- scenarios like military arms races, office resource allocation, and environmental conservation efforts.",
        "level": 3,
        "parentId": "methods"
      },
      {
        "id": "evaluation",
        "title": "Evaluation Metrics",
        "content": "Evaluation Metrics We define the total utility player over turns optimum both total scores and the difference between also measure qualitative , Cooperation , Communication Skills , Respect , and Consistency , each scored 1-10. Evaluation Metrics We define the total utility player P_i over T turns P_i k 1 T U P_i The social optimum both total scores and the difference between 2 P_1 , U P_2 (U P_1 U P_2 - U P_1 - U P_2",
        "level": 3,
        "parentId": "methods"
      },
      {
        "id": "results",
        "title": "Results",
        "content": "Results All experiments used LLaMA 3.2 3B and results are averaged over 4 seeds to account noise. Results All experiments used code",
        "level": 2
      },
      {
        "id": "synthetic-results",
        "title": "Synthetic Game Results",
        "content": "Synthetic Game Results First key game scenarios the Nash equilibrium is distinct from the social optimum, our moderator consistently improves fairness and cooperation. This includes Prisoner's Dilemma, Hawk-Dove, and Stag Hunt. Second key games the Nash equilibrium coincides the social Battle of the Sexes, Deadlock), the moderator doesn't significantly improve outcomes. This makes are already individually driven toward the social optimum, so moderation can't much. These results indicate that moderation plays critical role in fostering cooperative strategies individual rationality conflicts collective benefit. The moderator effectively provides neutral perspective that reminds agents of the collective benefit of cooperation and encourages trust. Our qualitative results show consistent improvement in agents' perception of each other across all trust, cooperation, communication, respect, consistency) moderator is present. This addresses our second research , agents do better perception of...",
        "level": 3,
        "parentId": "results"
      },
      {
        "id": "realistic-results",
        "title": "Realistic Game Results",
        "content": "Realistic Game Results As expected, it's more challenging LLM agents to perform in realistic game utility matrices aren't explicitly provided. Moderation generally improves scores in realistic , except Battle of the requires coordination across turns, something our current doesn't fully support). The difference is relatively limited since real- scenarios don't explicit payoff matrices, making it problem LLMs. But the trend is Realistic Game Results As expected, it's more challenging LLM agents to perform in realistic game utility matrices aren't explicitly provided. Moderation generally improves scores in realistic , except Battle of the requires coordination across turns, something our current doesn't fully support).",
        "level": 3,
        "parentId": "results"
      },
      {
        "id": "deal-results",
        "title": "Deal or No Deal Results",
        "content": "Deal or No Deal Results We also tested on the Deal or No Deal dataset , agents split an inventory books, balls) different Most conversations resulted in either disagreements or invalid item counts, to zero scores. There no significant quantitative difference moderator. However, qualitative metrics showed consistent enhancements , the most substantial improvement in trust. Deal or No Deal Results We also tested on the Deal or No Deal Lewis et al., 2017) , agents split an inventory books, balls) different Most conversations resulted in either disagreements or invalid item counts, to zero scores. Deal or No Deal qualitative scores showing improvement moderation",
        "level": 3,
        "parentId": "results"
      },
      {
        "id": "discussion",
        "title": "Discussion & Future Work",
        "content": "Discussion Future Work This study demonstrates that LLM- moderators can improve cooperation and fairness in multi-agent games, especially individual incentives conflict collective benefit. The development of the RealisticGames2x2 dataset also provides valuable resource investigating LLM behavior in strategic interactions. focuses on two-player, two-action games. Expanding to more complex multi-player, extensive-form, partial information) remains an open challenge. Additionally, attempts to train the moderator using PPO unsuccessful due to training instability from the probabilistic reward model. Future to more complex game-theoretic , improving moderator training alternative methodologies, and investigating improvements in fairness are uniformly distributed across demographic groups are all promising directions. Additionally, attempts to train the moderator using PPO unsuccessful due to training instability from the probabilistic reward model.",
        "level": 2
      },
      {
        "id": "references",
        "title": "References & Resources",
        "content": "References Resources Papers et al. - Playing repeated games Large Language Models Lewis et al. - Deal or No -to-End Learning of Negotiation Dialogues Abdelnabi et al. - LLM- LLMs Interactive Multi-Agent Negotiation Games Costarelli et al. - Strategic Reasoning Abilities of LLM Agents Bakhtin et al. - Human-level play in the game of Diplomacy Gandhi et al. - Strategic Reasoning Language Models Code Repository This done Harshvardhan Agarwal and Pranava Singhal as part of CS329x at Stanford. All authors contributed equally to this References Resources Papers et al. - Strategic Reasoning Language Models Code Repository",
        "level": 2
      }
    ]
  },
  {
    "title": "What Actually Happens Inside LLMs When You Use RL?",
    "url": "/blog/understanding-rl-internal-representations",
    "description": "We peeked under the hood to see how reinforcement learning changes what's going on inside language models. Spoiler: it's way cooler than we thought.",
    "category": "Blog",
    "content": "So the knows that reinforcement learning makes language models better at reasoning. Models like OpenAI's DeepSeek-R1 absolutely crush math problems after RL training. But bugged inside the know RL but don't really know it Does it completely rewrite the model's it just tweak few it memorizing everything or actually learning to collaborator Rahul and I decided to find out. We took two open-source Qwen3-1.7B and LLaMA-3.2-1B), fine-tuned them on math problems using both supervised fine-tuning and specifically GRPO The This Matters Large language models become the foundation of modern NLP. Beyond standard pretraining, two main strategies emerged to specialize fine-tuning reinforcement learning SFT uses next-token prediction from demonstrations, RL methods like RLHF optimize behavior via scalar rewards, enabling models to follow instructions better, reduce outputs, and reason more effectively Recent RLHF-trained models shown massive improvements. OpenAI's o1 uses RL to generate long latent reasoning chains before responding and achieves breakthroughs on STEM benchmarks DeepSeek-R1 uses critic-free RL algorithm from scratch any supervised fine-tuning to achieve reasoning capabilities comparable to o1 Despite these advances, the inner of RL fine-tuning remain opaque. Existing research primarily evaluates final accuracy, latency, etc.) and neglects internal representations evolve. This is critical because RLHF algorithms typically include KL-divergence penalties intended to constrain drift from model, suggesting that internal structure is changing but in unclear Problems and Two Training Methods We chose We used two Qwen3-1. .7B parameter model unsloth Llama-3.2-1B- instruction-finetuned version of LLaMA-3.2- used the instruction-tuned version because the model is poor at instruction following, making it to verify answers GRPO) We trained both models 4 Supervised Fine-Tuning): The classic predict the next token from example solutions using cross-entropy loss RL Relative Policy Optimization estimates advantages by comparing sampled outputs group, avoiding the need separate critic model RL and KL see much that \"stay close to the model\" constraint matters GRPO is particularly cool because it cuts memory usage roughly in compared to PPO and simplifies training dynamics. Instead of training separate value , it estimates advantages by comparing outputs group- comparison aligns reward- ranking and enabled DeepSeek-R1-Zero to learn reasoning strategies via RL alone supervised The Crushes But Why?) First, the obvious performed better. On Qwen, RL KL 83.55 accuracy 70 SFT. That's 20 On LLaMA, the even 24 The model scores 20.77 Qwen and 48.67 LLaMA, so both methods improved things, but RL did better. Method Eval Epoch Qwen3-1.7B RL Qwen3-1.7B no KL) Qwen3-1.7B SFT LLaMA-3.2 1B RL LLaMA-3.2 1B no KL) LLaMA-3.2 1B SFT Base model 20.77 , LLaMA 48.67 Green rows RL methods. RL models also generated longer completions, often using the full 512-token budget. At first, I thought this but it turns out they're being different solution paths and then confidently landing on the answer. This is phenomenon that's been observed before in the DeepSeek-R1 incentivizes exploration, often leads to longer and better answers. Qwen, training and KL-divergence resulted in no distinguishable difference in final performance. LLaMA, training KL-divergence led to better 7 Interestingly, LLaMA-3.2 1B, SFT actually made the model than the This is likely because the model itself been fine-tuned long duration. But it gets at changed inside the models, RL barely moved anything. SFT, on the other High-Level Changed in the Far Did We measured far the fine-tuned drifted from the model using L2 distance. The results caused massive drift. The moved further from the original model. RL models stayed surprisingly close to the Even more KL regularization stayed even closer, but RL KL still did better than SFT staying closer to the Rank Much Information Are We also looked at the \"rank\" ...",
    "sections": [
      {
        "id": "context",
        "title": "The Context: Why This Matters",
        "content": "The This Matters Large language models become the foundation of modern NLP. Beyond standard pretraining, two main strategies emerged to specialize fine-tuning reinforcement learning SFT uses next-token prediction from demonstrations, RL methods like RLHF optimize behavior via scalar rewards, enabling models to follow instructions better, reduce outputs, and reason more effectively Recent RLHF-trained models shown massive improvements. OpenAI's o1 uses RL to generate long latent reasoning chains before responding and achieves breakthroughs on STEM benchmarks DeepSeek-R1 uses critic-free RL algorithm from scratch any supervised fine-tuning to achieve reasoning capabilities comparable to o1 Despite these advances, the inner of RL fine-tuning remain opaque. Existing research primarily evaluates final accuracy, latency, etc.) and neglects internal representations evolve. This is critical because RLHF algorithms typically include KL-divergence penalties intended to constrain drift from model...",
        "level": 2
      },
      {
        "id": "setup",
        "title": "The Setup: Math Problems and Two Training Methods",
        "content": "Problems and Two Training Methods We chose We used two Qwen3-1. .7B parameter model unsloth Llama-3.2-1B- instruction-finetuned version of LLaMA-3.2- used the instruction-tuned version because the model is poor at instruction following, making it to verify answers GRPO) We trained both models 4 Supervised Fine-Tuning): The classic predict the next token from example solutions using cross-entropy loss RL Relative Policy Optimization estimates advantages by comparing sampled outputs group, avoiding the need separate critic model RL and KL see much that \"stay close to the model\" constraint matters GRPO is particularly cool because it cuts memory usage roughly in compared to PPO and simplifies training dynamics. Instead of training separate value , it estimates advantages by comparing outputs group- comparison aligns reward- ranking and enabled DeepSeek-R1-Zero to learn reasoning strategies via RL alone supervised Problems and Two Training Methods We chose grade school math problems) few r...",
        "level": 2
      },
      {
        "id": "results",
        "title": "The Results: RL Crushes It (But Why?)",
        "content": "The Crushes But Why?) First, the obvious performed better. On Qwen, RL KL 83.55 accuracy 70 SFT. That's 20 On LLaMA, the even 24 The model scores 20.77 Qwen and 48.67 LLaMA, so both methods improved things, but RL did better. Method Eval Epoch Qwen3-1.7B RL Qwen3-1.7B no KL) Qwen3-1.7B SFT LLaMA-3.2 1B RL LLaMA-3.2 1B no KL) LLaMA-3.2 1B SFT Base model 20.77 , LLaMA 48.67 Green rows RL methods. RL models also generated longer completions, often using the full 512-token budget. At first, I thought this but it turns out they're being different solution paths and then confidently landing on the answer. This is phenomenon that's been observed before in the DeepSeek-R1 incentivizes exploration, often leads to longer and better answers. Qwen, training and KL-divergence resulted in no distinguishable difference in final performance. LLaMA, training KL-divergence led to better 7 Interestingly, LLaMA-3.2 1B, SFT actually made the model than the This is likely because the model itself been fine-...",
        "level": 2
      },
      {
        "id": "high-level",
        "title": "High-Level Analysis: What Changed in the Weights?",
        "content": "High-Level Changed in the Weights?",
        "level": 2
      },
      {
        "id": "l2-distance",
        "title": "L2 Distance: How Far Did We Drift?",
        "content": "L2 Far Did We measured far the fine-tuned drifted from the model using L2 distance. The results caused massive drift. The moved further from the original model. RL models stayed surprisingly close to the Even more KL regularization stayed even closer, but RL KL still did better than SFT staying closer to the L2 Far Did We measured far the fine-tuned drifted from the model using L2 distance. The results shocking: Image L2 distance comparison showing RL stays closer to model",
        "level": 3,
        "parentId": "high-level"
      },
      {
        "id": "rank-change",
        "title": "Rank Change: How Much Information Are We Storing?",
        "content": "Rank Much Information Are We also looked at the \"rank\" of the much information they store). rank means more \"information\", lower rank means the matrix stores less \"information\" since it can be decomposed into one only loss. To measure this, first found the rank of the model accounts 99 of information using SVD. example, Q 2048 x 2048), K might be 2000. We then found out much information the same K top eigenvalues preserve in the fine- RL SFT) and subtracted the two negative value means the rank increased during the same vectors now less information than they did previously). SFT training led to rank than the models. RL models preserved or even reduced the rank. This suggests SFT is memorizing more information, RL is learning more general patterns. Think of it like you're teaching someone to add numbers. SFT is like making them memorize \"2 2 4, 4 4 8, 8 10 18...\" every possible combination. RL is like teaching them the general concept of addition so they can figure out any problem. The ...",
        "level": 3,
        "parentId": "high-level"
      },
      {
        "id": "token-level",
        "title": "Token-Level Analysis: Where Does the Model Look?",
        "content": "Token-Level Does the Model Look?",
        "level": 2
      },
      {
        "id": "attention-matrices",
        "title": "Self-Attention Matrices: The Big Picture",
        "content": "Self-Attention Big Picture We plotted the difference between the self-attention matrix of the fine-tuned RL SFT) and the model particular prompt. Similar to the analysis, both the self-attention pattern and the scores remained much closer in \"RL vs. Pre\" than in \"SFT vs. Important Note The scales \"SFT vs. Pre\" much 0.1 to - than the scales the RL rows. So visually it might look like \"RL vs. Pre\" similar differences, but it's not the case. SFT causes bigger changes. Interestingly, the difference between KL and no KL is more especially in LLaMA), self-attention scores of models trained KL, as expected, remain closer to the model. Similar to the analysis, both the self-attention pattern and the scores remained much closer in",
        "level": 3,
        "parentId": "token-level"
      },
      {
        "id": "per-token",
        "title": "Per-Token Attention: What Gets Focused On?",
        "content": "Per-Token Gets Focused visualized the difference in attention received by each token in the prompt between the and fine-tuned models. given self-attention matrix , represents the attention paid by token to token , computed the total attention received by each token as the column- sum To mitigate the influence of the first often acts as an attention sink and normalized the remaining vector. One observation that remains consistent across models and pays much more attention to the formatting GRPO, the reward strictly depends on the right answer being , so the model tries to always output it and also outputs it multiple often repeating the answer) Given know from other e. ., L2 difference) and our that RL-trained models only slightly nudge the model in the right direction, the per-token attention pattern means that RL pays more attention to just few tokens, and these tokens are sufficient improving performance Another observation that supports look at the attention difference in tokens mak...",
        "level": 3,
        "parentId": "token-level"
      },
      {
        "id": "entropy",
        "title": "The Entropy Story: Exploration vs. Overconfidence",
        "content": "The Entropy vs. Overconfidence This my favorite finding. We tracked the \"entropy\" of the model as it generates each token. High entropy the model is exploring, low entropy it's confident. More specifically, e is calculated the softmax probability of the -th vocabulary the -th token. flatter, more uniform distribution means negative value of sharper peaked only one is probable) means be closer to 0. broadly, then gets confident early, then uncertain There are two key things -trained models are more exploratory in the initial phase As RL models get closer to the answer, they gradually become more confident The Entropy vs. We tracked the of the model as it generates each token. More specifically, e t is calculated t v 1 V t,v log t,v t,v is the softmax probability of the v -th vocabulary the t -th token. flatter, more uniform distribution means negative value of e t sharper peaked only one is probable) means e t be closer to 0. Image RL model output entropy overlay RL entropy plot showing...",
        "level": 2
      },
      {
        "id": "discussion",
        "title": "Discussion & Limitations",
        "content": "Discussion Limitations Our study focuses on two open-source models and single math benchmark, may limit generalization to other domains or model scales. our structural analyses provide clear trends, they do not capture all possible forms of internal drift. Future could expand to additional architectures, tasks, and interpretability tools broader validation. broader impact of the project is that most RL research usually focuses on better algorithms, evaluations, and environments. Our project moves in different direction and us understand RL Discussion Limitations Our study focuses on two open-source models and single math benchmark, may limit generalization to other domains or model scales. Future could expand to additional architectures, tasks, and interpretability tools broader validation.",
        "level": 2
      },
      {
        "id": "what-means",
        "title": "What This All Means",
        "content": "What This All Means So the big doesn't rewrite the model. It makes targeted changes that preserve the model's reasoning abilities. SFT memorizes. It stores more information but doesn't generalize as RL encourages exploration. Models stay uncertain longer, try different paths, and then confidently commit to answers. Base models already know to reason. RL just activates the right circuits and sharpens focus on matters. RL preserves attention structure. Especially KL regularization, attention patterns stay close to the model. RL focuses on key tokens. It pays more attention to formatting tokens like needing to change attention to problem content. This implications alignment and interpretability. RL makes interpretable changes rather than massive rewrites, can better understand and control models are doing. We can design training that preserves the good stuff fixing the bad stuff. It pays more attention to formatting tokens like boxed needing to change attention to problem content.",
        "level": 2
      },
      {
        "id": "takeaway",
        "title": "The Takeaway",
        "content": "The Takeaway RL fine-tuning is like precision makes surgical changes rather than swinging sledgehammer. It nudges models in the right direction breaking already And that's probably it's so effective at improving reasoning maintaining generalization. These results suggest that RL fine-tuning encourages efficient and selective adaptation, SFT drives more global but less structured changes. Understanding these internal differences is key to designing future alignment strategies that balance performance interpretability. It nudges models in the right direction breaking already And that's probably it's so effective at improving reasoning maintaining generalization.",
        "level": 2
      },
      {
        "id": "references",
        "title": "References & Resources",
        "content": "References Resources Papers et al. - Training language models to follow instructions feedback Shao et al. - the Limits of Mathematical Reasoning DeepSeek-AI - DeepSeek- Reasoning Capability in LLMs via Reinforcement Learning Cobbe et al. - Training Verifiers to Solve Math Word Problems Barbero et al. - Why do LLMs attend to the first - o1 System Card Code Repository Full Plots Visualizations This done Rahul Chand as part of CS224R at Stanford. Shoutout to the open-source community making models like Qwen and LLaMA available research. - Why do LLMs attend to the first - o1 System Card Code Repository Full Plots Visualizations",
        "level": 2
      }
    ]
  },
  {
    "title": "Research — Arpan Khatua",
    "url": "/research",
    "description": "",
    "category": "Page",
    "content": ""
  },
  {
    "title": "Research Assistant",
    "url": "/experience",
    "description": "Experience Work that shaped I build. Academic Where I learned to learn.",
    "category": "Page",
    "content": "Experience Work that shaped I build. Academic Where I learned to learn. Coursework Teaching Conversational Virtual Assistants, Human Centered NLP, Reinforcement Learning, Deep RL Teaching Conversational Virtual Assistants) Coursework Teaching Senior Large Real World and Synthetic Graph Datasets GNN Applications , ML, Deep Learning, Algorithms, Parallel Programming, Data Structures, Database Systems, Computer Systems Computer Systems), ECE 313 , ECE 210 , ECE 110 Honors Lab Highest Honors, Bronze Tablet, Edmund J. James Scholar that shaped I build. Where I learned to learn. Coursework Teaching Conversational Virtual Assistants, Human Centered NLP, Reinforcement Learning, Deep RL Teaching Conversational Virtual Assistants) Coursework Teaching Senior Large Real World and Synthetic Graph Datasets GNN Applications , ML, Deep Learning, Algorithms, Parallel Programming, Data Structures, Database Systems, Computer Systems Computer Systems), ECE 313 , ECE 210 , ECE 110 Honors Lab"
  },
  {
    "title": "Projects — Arpan Khatua",
    "url": "/projects",
    "description": "Projects Things I built to learn faster. I built to learn faster.",
    "category": "Page",
    "content": "Projects Things I built to learn faster. I built to learn faster."
  },
  {
    "title": "Contact — Arpan Khatua",
    "url": "/contact",
    "description": "Contact Say Reach out collaborations, experiments, or to swap notes on ML and tooling. Email LinkedIn GitHub Crafted mix of sharp sans and soft serif Prime Intellect edge, Claude calm.",
    "category": "Page",
    "content": "Contact Say Reach out collaborations, experiments, or to swap notes on ML and tooling. Email LinkedIn GitHub Crafted mix of sharp sans and soft serif Prime Intellect edge, Claude calm. Reach out collaborations, experiments, or to swap notes on ML and tooling. Email LinkedIn GitHub"
  },
  {
    "title": "Blog — Arpandeep Khatua",
    "url": "/blog",
    "description": "Blog Notes on ML, tooling, and building.",
    "category": "Page",
    "content": "Blog Notes on ML, tooling, and building."
  },
  {
    "title": "Bio",
    "url": "/",
    "description": "Hi I'm Arpan I broadly on problems related to language agents, model evaluation, and making LLMs play nice and each other. I'm MSCS student at Stanford , I'm blessed amazing advisors Prof.",
    "category": "Page",
    "content": "Hi I'm Arpan I broadly on problems related to language agents, model evaluation, and making LLMs play nice and each other. I'm MSCS student at Stanford , I'm blessed amazing advisors Prof. Diyi Yang Prof. Jure Leskovec Prof. Monica Lam Before that I at I people around the connect on Facebook groups. I built systems to keep groups safe and friendly and LLM agents to answer people's questions. I completed my BS in Computer Engineering from UIUC , I on large-scale graph systems the amazing Vikram Sharma Mailthody Prof. Wen-Mei Hwu GitHub Twitter LinkedIn Email Arpan I broadly on problems related to language agents, model evaluation, and making LLMs play nice and each other. Jure Leskovec , and Prof. Monica Lam Before that I at , I on large-scale graph systems the amazing Vikram Sharma Mailthody and Prof. Wen-Mei Hwu CV"
  },
  {
    "title": "Detecting Corpus-Level Knowledge Inconsistencies in Wikipedia with Large Language Models",
    "url": "/research",
    "description": "Stanford researchers have discovered that at least 3.3 percent of English Wikipedia facts contradict other information in the corpus. This amounts to millions of inconsistent statements across the encyclopedia. The team introduced CLAIRE, an AI system that helps Wikipedia editors identify 64.7 percent more inconsistencies than traditional methods.",
    "category": "Research",
    "content": "Study Reveals 3.3% of Wikipedia Facts Contradict Each Other Stanford researchers have discovered that at least 3.3 percent of English Wikipedia facts contradict other information in the corpus. This amounts to millions of inconsistent statements across the encyclopedia. The team introduced CLAIRE, an AI system that helps Wikipedia editors identify 64.7 percent more inconsistencies than traditional methods. Semnani, S. J., Burapacheep, J., Khatua, A., Atchariyachanvanit, T., Wang, Z., & Lam, M. S. Detecting Corpus-Level Knowledge Inconsistencies in Wikipedia with Large Language Models EMNLP 2025"
  },
  {
    "title": "VideoWeave: A Data-Centric Approach for Efficient Video Understanding",
    "url": "/research",
    "description": "A data-centric approach for efficient video understanding achieved 3 percent absolute improvement on VideoMME benchmarks under identical compute constraints, without requiring architectural modifications. The method splices short captioned videos into synthetic long-context training samples.",
    "category": "Research",
    "content": "Video Understanding System Achieves 3% Improvement Without Architecture Changes A data-centric approach for efficient video understanding achieved 3 percent absolute improvement on VideoMME benchmarks under identical compute constraints, without requiring architectural modifications. The method splices short captioned videos into synthetic long-context training samples. Durante, Z., Singh, S., Khatua, A., Agarwal, S., Tan, R., Lee, Y. J., Gao, J., Adeli, E., & Fei-Fei, L. VideoWeave: A Data-Centric Approach for Efficient Video Understanding NeurIPS 2025 Workshop (Oral)"
  },
  {
    "title": "VideoMultiAgents: A Multi-Agent Framework for Video Question Answering",
    "url": "/research",
    "description": "A multi-agent framework for video question answering with role specialization improved zero-shot performance by up to 6 percent over prior state-of-the-art methods through collaborative reasoning and information aggregation.",
    "category": "Research",
    "content": "Multi-Agent Video QA System Improves Zero-Shot Performance by 6% A multi-agent framework for video question answering with role specialization improved zero-shot performance by up to 6 percent over prior state-of-the-art methods through collaborative reasoning and information aggregation. Kugo, N., Li, X., Li, Z., Gupta, A., Khatua, A., Jain, N., Patel, C., Kyuragi, Y., Ishii, Y., Tanabiki, M., Kozuka, K., & Adeli, E. VideoMultiAgents: A Multi-Agent Framework for Video Question Answering Panasonic"
  },
  {
    "title": "IGB: Addressing The Gaps In Labeling, Features, Heterogeneity, and Size of Public Graph Datasets for Deep Learning Research",
    "url": "/research",
    "description": "Researchers created the Illinois Graph Benchmark (IGB), the largest academic GNN dataset with 260 million nodes, 4 billion edges, and 220 million labels. The dataset is 162 times larger than prior datasets and has been adopted as an MLPerf industry standard for GNN benchmarking.",
    "category": "Research",
    "content": "Largest Academic Graph Dataset Created: 162× Larger Than Prior Benchmarks Researchers created the Illinois Graph Benchmark (IGB), the largest academic GNN dataset with 260 million nodes, 4 billion edges, and 220 million labels. The dataset is 162 times larger than prior datasets and has been adopted as an MLPerf industry standard for GNN benchmarking. Khatua, A., Mailthody, V., Taleka, B., Song, X., Ma, T., Bigaj, P. & Hwu, W. IGB: Addressing The Gaps In Labeling, Features, Heterogeneity, and Size of Public Graph Datasets for Deep Learning Research KDD 2023"
  },
  {
    "title": "The Curse of Coordination: Why Current AI Cannot be Your Teammates",
    "url": "/research",
    "description": "Initial results from Cotomata, a multi-agent collaboration benchmark, show that LLM coordination in version-controlled programming environments suffers dramatically. Models drop from 70-75% accuracy (single-agent) to 20-30% (multi-agent), with majority of failures attributed to mismatched shared-state assumptions. The benchmark includes multi-agent interaction protocols and a failure analysis pipeline.",
    "category": "Research",
    "content": "New Benchmark Reveals Coordination as Fundamental Challenge for Multi-Agent AI Systems Initial results from Cotomata, a multi-agent collaboration benchmark, show that LLM coordination in version-controlled programming environments suffers dramatically. Models drop from 70-75% accuracy (single-agent) to 20-30% (multi-agent), with majority of failures attributed to mismatched shared-state assumptions. The benchmark includes multi-agent interaction protocols and a failure analysis pipeline. Khatua, A.¹, Zhu, H.¹, Tran, P.², Prabhudesai, A.², Yu, X.¹, Sadrieh, F.², Lieberwirth, J. K.², Fu, Y.¹, Ryan, M. J.¹, Pei, J.¹, & Yang, D.¹ (¹Stanford University, ²SAP) The Curse of Coordination: Why Current AI Cannot be Your Teammates In Progress (Targeting ICML 2026)"
  },
  {
    "title": "HumanLM: Building Digital Humans from Large Language Models",
    "url": "/research",
    "description": "HumanLM introduces hierarchical RL modules for user simulation, with initial results showing improved alignment with real user responses. The system raises LLM-judge similarity by 36% over SFT baselines through multi-level generation training and carefully designed stance and style reward functions.",
    "category": "Research",
    "content": "Hierarchical RL User Simulation Improves Alignment by 36% HumanLM introduces hierarchical RL modules for user simulation, with initial results showing improved alignment with real user responses. The system raises LLM-judge similarity by 36% over SFT baselines through multi-level generation training and carefully designed stance and style reward functions. Khatua, A.*, Wu, S.*, Choi, E.*, Wang, H., He-Yueva, J., Weerasooriya, C., Wei, W., Yang, D., Leskovec, J., & Zou, J.* HumanLM: Building Digital Humans from Large Language Models In Progress (Targeting ICML 2026)"
  },
  {
    "title": "Multilingual SWE-smith",
    "url": "/research",
    "description": "Extended SWE-smith by automating test environments construction for JavaScript/TypeScript, Java, Rust, and C++. The system scales procedural bug generation to any repository, producing mid-training data for improving Multi-SWE-Bench performance across multiple programming languages.",
    "category": "Research",
    "content": "Multilingual SWE-smith Scales Bug Generation Across Programming Languages Extended SWE-smith by automating test environments construction for JavaScript/TypeScript, Java, Rust, and C++. The system scales procedural bug generation to any repository, producing mid-training data for improving Multi-SWE-Bench performance across multiple programming languages. Khatua, A.*, Li, X.*, Shethia, P.*, Li, Z.*, & Yang, J. Multilingual SWE-smith In Progress (Targeting ICML 2026)"
  }
]